# Multi-Agent GitHub README Updater

> A Flask web application that uses a multi-agent AI pipeline with Ollama to automatically generate professional, structured README files for your GitHub repositories.

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![Flask](https://img.shields.io/badge/Flask-3.0+-green.svg)](https://flask.palletsprojects.com/)
[![Ollama](https://img.shields.io/badge/Ollama-llama3.1-orange.svg)](https://ollama.ai/)

---
## üìã Table of Contents

- [Overview](#-overview)
- [Features](#-features)
- [Architecture](#-architecture)
- [Tech Stack](#-tech-stack)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [How It Works](#-how-it-works)
- [Contributing](#-contributing)

---

## üéØ Overview

Tired of writing and maintaining README files for all your repositories? This tool automates the process using a sophisticated multi-agent AI system that:

1. üîó **Connects** to your GitHub account (Personal Access Token)
2. üìö **Analyzes** repository structure, code, dependencies, and frameworks
3. üíæ **Backs up** existing READMEs before making changes
4. ü§ñ **Generates** professional, structured documentation using AI
5. üîç **Reviews** and improves quality with a reviewer agent
6. üíª **Commits** directly to your repositories

Perfect for maintaining consistent documentation across personal or organizational repositories.

---

## ‚ú® Features

### Core Functionality
- üîê **Simple Authentication** - GitHub Personal Access Token (no OAuth complexity)
- üìã **Repository Management** - View all repos with stats (stars, forks, language, last update)
- ‚úÖ **Select All** - Bulk select repositories with filtering
- üîç **Advanced Filtering** - Filter by language, visibility (public/private), README status
- üíæ **Automatic Backups** - Local backup before any changes (restore anytime)
- üëÄ **Preview Mode** - Review generated README before committing

### AI-Powered Generation
- ü§ñ **Multi-Agent Pipeline** - Three specialized LLM agents collaborate
- üß† **Intelligent Analysis** - Understands project type, purpose, and architecture
- ‚úçÔ∏è **Smart Generation** - Creates contextual, relevant documentation
- üîç **Quality Review** - Automated review and improvement pass
- üìä **Quality Scoring** - Rates generated README quality (0-100)

### User Experience
- üé® **Modern UI** - Clean Bootstrap 5 interface with gradients
- üì± **Responsive Design** - Works on desktop and mobile
- ‚ö° **Real-time Feedback** - Progress indicators during generation
- üìù **Edit Before Commit** - Modify generated content in preview
- üîÑ **Restore Backups** - Easy one-click restoration from web UI

---

## üèóÔ∏è Architecture

This project follows a **multi-agent architecture** where specialized AI agents collaborate through a pipeline:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Services Layer                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  GitHub    ‚îÇ  ‚îÇ   Backup    ‚îÇ  ‚îÇ     LLM Service    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  Service   ‚îÇ  ‚îÇ   Service   ‚îÇ  ‚îÇ Ollama or OpenAI   ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Multi-Agent Pipeline (LLM-Powered)              ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ   1Ô∏è‚É£ Analyzer Agent ‚Üí 2Ô∏è‚É£ Generator Agent ‚Üí 3Ô∏è‚É£ Reviewer Agent ‚îÇ
‚îÇ      üß† Understands      ‚úçÔ∏è Creates           üîç Improves    ‚îÇ
‚îÇ      project type        README content       quality        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Agent Responsibilities

| Agent | Type | Purpose |
|-------|------|---------|
| **AnalyzerAgent** | AI Agent (LLM) | Analyzes repository to understand project type, purpose, key features, target audience |
| **GeneratorAgent** | AI Agent (LLM) | Generates comprehensive README with proper structure and sections |
| **ReviewerAgent** | AI Agent (LLM) | Reviews generated content, removes placeholders, improves formatting |
| DiscoveryService | Utility Service | Filters and sorts repositories (no LLM) |
| WriterService | Utility Service | Handles Git commits and PRs (no LLM) |

---

## üõ†Ô∏è Tech Stack

| Component | Technology | Version |
|-----------|------------|---------|
| **Backend** | Flask | 3.0.0 |
| **Frontend** | Bootstrap 5 + Jinja2 | 5.3.0 |
| **GitHub API** | PyGithub | 2.1.1 |
| **LLM (Default)** | Ollama (llama3.1:8b) | Latest |
| **LLM (Optional)** | OpenAI GPT-4 / GPT-3.5 | Latest |
| **Language** | Python | 3.11+ |

---

## üì¶ Installation

### Prerequisites

- Python 3.11 or higher
- **For Ollama (Default)**: [Ollama](https://ollama.ai/) installed and running
- **For OpenAI (Optional)**: OpenAI API key
- GitHub Personal Access Token with `repo` scope

### Setup Steps

```bash
# Clone the repository
git clone https://github.com/mutabay/Multi-Agent-GitHub-README-Updater.git
cd Multi-Agent-GitHub-README-Updater

# Create virtual environment
python -m venv .venv

# Activate virtual environment
# Windows:
.venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# For Ollama: Install the model (if not already installed)
ollama pull llama3.1:8b

# For OpenAI: Install the OpenAI package
# pip install openai
```

> **Note**: A `.env.example` file is provided as a template. Copy it to `.env` and configure as needed.

---

## ‚öôÔ∏è Configuration

Create a `.env` file in the root directory (optional - has sensible defaults):

### Option 1: Using Ollama (Default - Local & Free)

```env
# LLM Provider
LLM_PROVIDER=ollama

# Ollama Configuration
OLLAMA_MODEL=llama3.1:8b
OLLAMA_BASE_URL=http://localhost:11434

# Flask
SECRET_KEY=your-secret-key-for-sessions
```

### Option 2: Using OpenAI GPT (Cloud-based)

```env
# LLM Provider
LLM_PROVIDER=openai

# OpenAI Configuration
OPENAI_API_KEY=sk-your-api-key-here
OPENAI_MODEL=gpt-4  # or gpt-3.5-turbo

# Flask
SECRET_KEY=your-secret-key-for-sessions
```

**To switch providers**: Simply change `LLM_PROVIDER` in `.env` and restart the app.

**Installing OpenAI support** (if using OpenAI):
```bash
pip install openai
```

### Getting a GitHub Token

1. Go to GitHub Settings ‚Üí Developer settings ‚Üí Personal access tokens ‚Üí Tokens (classic)
2. Click "Generate new token (classic)"
3. Give it a name (e.g., "README Updater")
4. Select scope: `repo` (full control of private repositories)
5. Generate and copy the token
6. Paste it when connecting in the web UI

---

## üöÄ Usage

### Starting the Application

```bash
# Make sure Ollama is running
ollama serve

# In another terminal, start the Flask app
python app.py
```

The application will be available at **http://127.0.0.1:5000**

### Workflow

1. **Connect** - Paste your GitHub Personal Access Token
2. **Browse** - View all your repositories with filters
3. **Select** - Choose repos to update (or use "Select All")
4. **Generate** - AI agents analyze and create READMEs
5. **Preview** - Review and edit generated content
6. **Commit** - Save directly to repositories

### Backup Management

All existing READMEs are automatically backed up to `backups/` folder:
- Format: `{username}_{repo}_{timestamp}.md`
- Accessible via `/backups` route in web UI
- One-click restore functionality

---

## üìÅ Project Structure

```
Multi-Agent-GitHub-README-Updater/
‚îú‚îÄ‚îÄ app.py                 # Flask application entry point
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ README.md              # This file
‚îÇ
‚îú‚îÄ‚îÄ agents/                # AI Agents (LLM-Powered)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ analyzer.py        # Analyzes repo type and features
‚îÇ   ‚îú‚îÄ‚îÄ generator.py       # Generates README content
‚îÇ   ‚îú‚îÄ‚îÄ reviewer.py        # Reviews and improves quality
‚îÇ   ‚îú‚îÄ‚îÄ discover.py        # DiscoveryService - repo filtering
‚îÇ   ‚îî‚îÄ‚îÄ writer.py          # WriterService - Git operations
‚îÇ
‚îú‚îÄ‚îÄ services/              # Non-LLM Services
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ github_service.py  # GitHub API integration
‚îÇ   ‚îú‚îÄ‚îÄ backup_service.py  # README backup management
‚îÇ   ‚îî‚îÄ‚îÄ llm_service.py     # Ollama LLM interface
‚îÇ
‚îú‚îÄ‚îÄ backups/               # Automatic backup storage
‚îÇ   ‚îî‚îÄ‚îÄ {username}_{repo}_{timestamp}.md
‚îÇ
‚îú‚îÄ‚îÄ templates/             # Jinja2 HTML templates
‚îÇ   ‚îú‚îÄ‚îÄ base.html          # Base layout with navbar
‚îÇ   ‚îú‚îÄ‚îÄ index.html         # Home - GitHub token input
‚îÇ   ‚îú‚îÄ‚îÄ repos.html         # Repository selection page
‚îÇ   ‚îú‚îÄ‚îÄ preview.html       # Generated README preview
‚îÇ   ‚îî‚îÄ‚îÄ result.html        # Success/error messages
‚îÇ
‚îú‚îÄ‚îÄ static/                # Frontend assets
‚îÇ   ‚îú‚îÄ‚îÄ css/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ style.css      # Custom styles
‚îÇ   ‚îî‚îÄ‚îÄ js/
‚îÇ       ‚îî‚îÄ‚îÄ app.js         # Client-side interactions
‚îÇ
‚îî‚îÄ‚îÄ tests/
    ‚îî‚îÄ‚îÄ test_agents.py     # Unit tests for agents
```

---

## üîß How It Works

### Phase 1: Repository Discovery
**DiscoveryService** (non-LLM) fetches all repositories from GitHub and applies filters:
- Active repos (pushed within last year)
- Non-fork repositories
- Significant size (>10KB)

### Phase 2: Analysis
**AnalyzerAgent** (LLM) performs deep project analysis:
- Reads and understands code structure
- Identifies project type (web app, CLI tool, library, etc.)
- Extracts key features and technologies
- Determines target audience

### Phase 3: Generation
**GeneratorAgent** (LLM) creates comprehensive README:
- Uses analysis insights to write contextual content
- Structures with proper markdown sections
- Adapts tone for personal vs. organizational repos
- Avoids generic placeholders like "Unknown"

### Phase 4: Review
**ReviewerAgent** (LLM) performs quality control:
- Checks for placeholder text ("Unknown", "TODO")
- Removes corporate language in personal repos
- Ensures proper formatting and consistency
- Validates content length (not too short or verbose)

### Phase 5: Commit
**WriterService** (non-LLM) handles Git operations:
- Creates backup of existing README
- Commits directly to repository
- Provides rollback via backup restoration

---

## ü§ù Contributing

Contributions are welcome! If you'd like to improve this project:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/improvement`)
3. Make your changes
4. Test thoroughly
5. Commit with clear messages (`git commit -m 'Add feature X'`)
6. Push to your fork (`git push origin feature/improvement`)
7. Open a Pull Request

### Development Guidelines
- Follow PEP 8 style guidelines
- Add docstrings to new functions
- Test with multiple repository types
- Keep LLM prompts clear and specific

---

## üìù License

This project is open source and available under the MIT License.

---

## üë§ Author

**Bayram M√ºtevellioƒülu**
- GitHub: [@mutabay](https://github.com/mutabay)

---

## ‚ö†Ô∏è Troubleshooting

### Common Issues

**"Model llama3.1:8b not found"**
```bash
ollama pull llama3.1:8b
```

**"Connection refused to Ollama"**
- Make sure Ollama is running: `ollama serve`
- Check Ollama is at http://localhost:11434

**"GitHub API rate limit exceeded"**
- Authenticated requests have 5000/hour limit
- Wait for rate limit reset or use different token

**Generation takes too long**
- Default timeout is 120 seconds
- Large repos with many files may take longer
- Consider adjusting `max_tokens` in [services/llm_service.py](services/llm_service.py)

---

## üîÆ Future Enhancements

- [ ] Batch processing with progress tracking
- [ ] Customizable README templates
- [ ] README version history and comparison

---
